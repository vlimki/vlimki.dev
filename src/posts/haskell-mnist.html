<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="dcterms.date" content="2024-07-15" />
  <meta name="description" content="Learning about neural networks, linear algebra, and Haskell with a hands-on project - training a neural network on MNIST from scratch." />
  <title>Handwritten Digit Recognition From Scratch in Haskell</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    svg {
      height: auto;
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Handwritten Digit Recognition From Scratch in
Haskell</h1>
<p class="date">2024-07-15</p>
</header>
<script>
  import Image from '$lib/components/Image.svelte';
</script>
<h2 id="notes">Notes</h2>
<p>Do I go like this: 1. Intro to NNs - what is a neural net?
Visualizations, theory, etc. 2. Forward prop (concept + math) &amp;
implementation 3. Backprop (concept + math) &amp; implementation 4.
Training on XOR 5. Improvements 6. Scaling &amp; training on MNIST</p>
<p>Or like this: 1. Intro to NNs - what is a neural net? Visualizations,
theory, etc. 2. Mathematics to forward propagation and backpropagation
3. Implementation of the entire network 4. Training on XOR 5.
Improvements 6. Scaling &amp; training on MNIST</p>
<p>The former probably.</p>
<p>ChatGPT elaboration:</p>
<ol type="1">
<li><strong>Introduction to Neural Networks</strong>
<ul>
<li>Overview of neural networks</li>
<li>Importance and applications</li>
<li>Basic structure and terminology</li>
<li>Brief review of linear algebra essentials (vectors, matrices,
etc.)</li>
<li>Activation functions and their roles</li>
</ul></li>
<li><strong>Forward Propagation</strong>
<ul>
<li>Concept and mathematical formulation</li>
<li>Step-by-step forward propagation in a neural network</li>
<li>Implementation of forward propagation in Haskell</li>
</ul></li>
<li><strong>Backpropagation</strong>
<ul>
<li>Concept and mathematical formulation</li>
<li>Detailed explanation of the gradient descent algorithm</li>
<li>Implementation of backpropagation in Haskell</li>
</ul></li>
<li><strong>Training on XOR</strong>
<ul>
<li>Explanation of the XOR problem</li>
<li>Training the neural network on the XOR dataset</li>
<li>Analyzing results and understanding errors</li>
</ul></li>
<li><strong>Improvements and Optimization</strong>
<ul>
<li>Techniques for improving neural network performance</li>
<li>Regularization, learning rate adjustments, etc.</li>
</ul></li>
<li><strong>Scaling Up: Training on MNIST</strong>
<ul>
<li>Introduction to the MNIST dataset</li>
<li>Preprocessing data for Haskell</li>
<li>Training the neural network on MNIST</li>
<li>Evaluating performance and discussing results</li>
</ul></li>
<li><strong>Conclusion and Future Directions</strong>
<ul>
<li>Summary of what was learned</li>
<li>Potential next steps and further reading</li>
</ul></li>
</ol>
<hr />
<h2 id="foreword">Foreword</h2>
<p>Neural networks are one of the most important cornerstones of machine
learning, hence it’s important to understand them extremely well, and be
capable of working with one. Hence I’ve dedicated so much time to
writing just this one post that summarizes most of the things I’ve
learned, as opposed to writing many smaller posts. It’s very important
to see how all the small things in a neural network work together, and
how they’re connected.</p>
<p>Arguably the best way to learn is to build things. Which is why I
took on the challenge of writing a neural network from scratch in
Haskell, and trained it on the MNIST handwritten digit recognition
dataset. A neural network is actually the perfect project for a machine
learning engineer; not only is it terribly important knowledge, but it’s
also <strong>perfectly</strong> challenging to implement.</p>
<p>In this post I’ll be going over all of the important neural network
knowledge: what is a neural net, why it is a thing, how it works, the
mathematics behind it, and an implementation in Haskell… Haskell!?</p>
<h3 id="why-haskell">Why Haskell?</h3>
<p>Certainly it’s not the most convenient language to be writing a
neural network in — but it sure is the most fun! Haskell is very
theoretical and great for expressing mathematics. Considering the fact
that a neural network is literally nothing but math, Haskell is a great
fit. I also chose it for personal reasons — mainly due to the fact that
I really like math, but I also believe that Haskell will greatly assist
me at majoring math in university. ___</p>
<h2 id="a-brief-introduction-to-neural-networks">A Brief Introduction to
Neural Networks</h2>
<div class="my-3" />

<h3 id="what-is-a-neural-network">What Is a Neural Network?</h3>
<p>A neural network is essentially a model that tries to mimic how the
human brain learns. It has layers of neurons that pass on data to other
layers to eventually arrive at some computation. We’ll look into how
this works shortly — and how the human brain relates to it.</p>
<p>Before we’ve only gone over <strong>shallow learning
algorithms</strong> (e.g. <a
href="https://vlimki.dev/writing/day3">linear regression</a> or <a
href="https://vlimki.dev/writing/day14">support vector machines</a>) —
neural networks on the other hand, are called <strong>deep learning
algorithms</strong>. What this essentially means is that they are much
more complex, and are better at finding non-linear and complex
relationships in data. Shallow learning algorithms are simpler, faster
and more convenient, but they really start struggling when data is not
linearly separable.</p>
<p>Most of the cool machine learning projects you see — self-driving
cars, image recognition, or large language models to name a few — are
based on deep learning algorithms. Usually real-world tasks are very
complicated, so heavier and scalable models are needed to compensate.
Neural networks are at the heart of deep learning algorithms. This is
what makes learning about them so important. Most of the important deep
learning algorithms are just derivatives of neural networks anyway. A
lot of them even contain the world neural network in them.</p>
<h3 id="how-are-neural-networks-structured-and-a-biology-lesson">How are
Neural Networks Structured? (And a Biology Lesson)</h3>
<p>As I mentioned before, a neural network tries to follow a similar
model for learning as the human brain does. Let’s look into how the
human brain learns first.</p>
<p>Our brain is an insanely complex organism — it has approximately
<strong>86 billion</strong> neurons. A <strong>neuron</strong> is the
basic unit in the brain — they’re specialized cells that have a job of
transferring information. They’re pass information to each other through
what’s called <strong>synapses</strong>, which are essentially just
bridges between the neurons. What learning means for the human brain is
the weakening or strengthening of these synaptic connections.</p>
<p><Image src="/images/posts/haskell-mnist/biological-neurons.jpg" text="Biological neurons (learnopencv.com)" /></p>
<ol type="1">
<li>The dendrites accept new information.</li>
<li>The neuron does some computation on the information.</li>
<li>The newly computed information goes through the axon to the
synapses, and over to the next neuron.</li>
</ol>
<p>Alright, how does a model like this get recreated artificially on a
computer? I think it’ll first be helpful to show a visualization of an
artificial neural network — and I bet most people with an interest for
ML have already seen a visualization like this:</p>
<p><Image src="/images/posts/haskell-mnist/ann.png" text="Visualization of an artificial neural network (ChatGPT/matplotlib)" /></p>
<p>Every one of those units is an artificial neuron and the lines drawn
between them represent synapses. Let’s take a closer look:</p>
<p><Image src="/images/posts/haskell-mnist/comparison.png" text="Comparison of an artificial and a biological neuron. (powerelectronicsnews.com)" /></p>
<p>So similarly as the biological neuron has dendrites that receive
inputs, the artificial neuron also receives a vector of inputs. The
strength of the synaptic connections is represented by the two
parameters we’re already familiar with — the <strong>weight
vectors</strong> <span
class="math inline"><em>w</em><sub>1</sub>…<em>w</em><sub><em>n</em></sub></span>
and the <strong>biases</strong> <span
class="math inline"><em>b</em><sub>1</sub>…<em>b</em><sub><em>n</em></sub></span>.
The neural network tries to find the optimal parameters <span
class="math inline"><em>w</em></span> and <span
class="math inline"><em>b</em></span> for every single neuron in the
network. That can take a while, especially when you have millions — or
<strong>trillions</strong> of parameters, like GPT-4 does. The network
we’ll be building for the MNIST problem has roughly <span
class="math inline">530, 000</span> parameters.</p>
<p>The <strong>linear function</strong> seen in the image is just the
same <span
class="math inline"><em>w</em> ⋅ <em>x</em> + <em>b</em></span>
expression we have seen in many other algorithms. The <strong>activation
function</strong> is what gives the neural network its ability to
perform so well even when data isn’t linearly separable. As a matter of
fact, if you removed the activation functions from neural networks,
you’d just end up with an overcomplicated linear regression (<a
href="https://towardsdatascience.com/why-neural-networks-have-activation-functions-9732e5405d4e">here
is an excellent article on why that is the case</a>)! Typical activation
functions are for example the logistic function (familiar from logistic
regression), ReLU, or tanh.</p>
<p>So at essence, a neural network is just an array of layers, and
layers in turn are an array of neurons. In a layer, the only thing
unique to a neuron is its weight vector <span
class="math inline"><strong>w</strong></span> and the bias scalar <span
class="math inline"><em>b</em></span>.</p>
<p>How would we represent this kind of structure in code? Well, let’s go
back to some linear algebra. In neural networks, input vectors are
usually represented as column vectors. This may seem quite unintuitive
since they are often stored as row vectors in a data frame, for
example.</p>
<p>Let’s start with the linear function. So, for us to be able to
evaluate the expression <span
class="math inline"><strong>w</strong> ⋅ <strong>x</strong></span>, we
need a row vector <span class="math inline"><strong>w</strong></span> of
the same length as the input vector <span
class="math inline"><strong>x</strong></span>. Remember that the input
vector will get passed on to every neuron in each layer, so we will need
<span class="math inline"><em>n</em></span> row vectors <span
class="math inline"><strong>w</strong><sub>1</sub>…<strong>w</strong><sub><em>n</em></sub></span>
for every layer, where <span class="math inline"><em>n</em></span> is
the number of neurons in the layer of interest. Consider <span
class="math inline"><em>m</em></span> as the length of the input vector.
So this far we have:</p>
<p><span class="math display">$$
\mathbf{w}_i \cdot \mathbf{x} + b_i = \begin{bmatrix}w_i^{(1)} \dots
w_i^{(m)} \end{bmatrix} \begin{bmatrix} x^{(1)} \\ \vdots \\ x^{(m)}
\end{bmatrix} + b_i
$$</span></p>
<p>for every neuron <span
class="math inline">{(<strong>w</strong><sub><strong>i</strong></sub>,<em>b</em><sub><em>i</em></sub>)}<sub><em>i</em> = 1</sub><sup><em>n</em></sup></span>.</p>
<p>However, representing weights and biases on a neuron basis in code is
a bit inconvenient. Hmm… what’s a good way of representing an array of
weight vectors? Well, a weight matrix! This way we can group the weight
vectors of every neuron just one structure per layer — the weight matrix
<span class="math inline"><strong>W</strong></span>. It’s the same thing
with biases — we can combine every bias value into a single bias vector
<span class="math inline"><strong>b</strong></span>.</p>
<p>We can now represent the linear function <span
class="math inline"><em>z</em><sub><em>l</em></sub></span> of the layer
<span class="math inline"><em>l</em></span> as the following expression:
<span class="math display">$$
z_l = \mathbf{W} \cdot \mathbf{x} + \mathbf{b} = \begin{bmatrix}
w_1^{(1)} &amp; \dots &amp; w_1^{(m)} \\ \vdots &amp; \ddots &amp;
\vdots \\ w_n^{(1)} &amp; \dots &amp; w_n^{(m)} \end{bmatrix}
\begin{bmatrix} x^{(1)} \\ \vdots \\ x^{(m)} \end{bmatrix} +
\begin{bmatrix}b_1 \\ \vdots \\ b_n \end{bmatrix}
$$</span></p>
<p>This is matrix-vector multiplication. The result of <span
class="math inline"><strong>W</strong> ⋅ <strong>x</strong></span> is
another column vector, and we just add the bias vector to that. Sounds
simple enough?</p>
<p>The activation part is simple. We can denote it as: <span
class="math display"><em>a</em><sub><em>l</em></sub> = <em>g</em>(<em>z</em>)</span>
where <span class="math inline"><em>g</em></span> is some activation
function. Note that we apply <span class="math inline"><em>g</em></span>
to <span class="math inline"><em>z</em></span> on an element basis.</p>
<p>Now that we have all of that covered, how would we actually represent
a neural network in code? Well, a network doesn’t need to be anything
more complicated than a list of layers. So let’s define ourselves a
layer type:</p>
<p><code>haskell showLineNumbers title="Layer Type Definition" -- The layer type. The parameters in the network are stored on a layer basis. -- weights     = weight matrix -- biases      = bias matrix  -- sz          = number of neurons -- activation  = the activation function data Layer = Layer   { weights :: Matrix R   , biases :: Matrix R   , sz :: Int   , activation :: (R -&gt; R)   }</code></p>
<p>Don’t worry about <code>R</code>, it’s just a type synonym for a
<code>Double</code> (and the letter R comes from the set of real numbers
<span class="math inline">ℝ</span>). In Haskell, functions are denoted
as <code>Argument Type -&gt; Argument Type -&gt; Return Type</code>. Our
activation function takes one real number as an argument and returns
another real number. Note that the bias vector is a matrix of size <span
class="math inline"><em>n</em> × 1</span> (<span
class="math inline"><em>n</em></span> rows and <span
class="math inline">1</span> column, so it’s a column vector). It could
totally well be a <code>Vector R</code>, but it’s easier to just put a
matrix there due to how matrix multiplication will be handled later.</p>
<p>Let’s also define a function called <code>activate</code>, which
takes a <code>Layer</code> and an input vector as parameters and returns
us the activation value <span
class="math inline"><em>g</em>(<em>z</em>)</span> for the layer.</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="ot">activate ::</span> <span class="dt">Matrix</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Matrix</span> <span class="dt">R</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>activate input layer <span class="ot">=</span> cmap (activation l) (weights layer <span class="op">&lt;&gt;</span> x <span class="op">+</span> biases layer)</span></code></pre></div>
<p>In Haskell, <code>::</code> is used to denote a type signature. The
second line is the function body itself. This function definition is
very simple. All you need to know to understand it is that
<code>&lt;&gt;</code> stands for matrix multiplication, and
<code>cmap</code> is a function that applies some function on a matrix
element-wise.</p>
<p>A neural network with just one layer and the logistic activation
function is just logistic regression. Let’s try this out with GHCi (an
interactive Haskell environment). Recall that the logistic activation
function is defined as <span class="math inline">$\frac{1}{1 +
e^{-z}}$</span>.</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> sigmoid x <span class="ot">=</span> <span class="dv">1</span> <span class="op">/</span> (<span class="dv">1</span> <span class="op">+</span> <span class="fu">exp</span> (<span class="op">-</span>x))</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> l <span class="ot">=</span> <span class="dt">Layer</span> { weights <span class="ot">=</span> (<span class="dv">1</span><span class="op">&gt;&lt;</span><span class="dv">1</span>) [<span class="fl">0.5</span>]</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>                , biases <span class="ot">=</span> (<span class="dv">1</span><span class="op">&gt;&lt;</span><span class="dv">1</span>) [<span class="dv">1</span>]</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>                , sz <span class="ot">=</span> <span class="dv">1</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>                , activation <span class="ot">=</span> sigmoid }</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span>input <span class="ot">=</span> (<span class="dv">1</span><span class="op">&gt;&lt;</span><span class="dv">1</span>) [<span class="fl">0.123</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span>activate l x</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>(<span class="dv">1</span><span class="op">&gt;&lt;</span><span class="dv">1</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a> [ <span class="fl">0.7429770928842059</span> ]</span></code></pre></div>
<p>Works as expected. This is just normal logistic regression where
<span class="math inline"><em>w</em> = 0.5</span> and <span
class="math inline"><em>b</em> = 1</span>. Note that the
<code>(n&gt;&lt;m)</code> notation is used for creating a matrix with
<span class="math inline"><em>n</em></span> rows and <span
class="math inline"><em>m</em></span> columns.</p>
<p>There is an important detail here — we initialized both the weight
matrix and the bias vector to be of size <span
class="math inline">1 × 1</span>. We concluded before that the weight
matrices and bias vectors must have a specific size; in particular, the
weight vector must be of size <span
class="math inline"><em>n</em> × <em>m</em></span>, where <span
class="math inline"><em>n</em></span> is the number of features in the
input vector, and <span class="math inline"><em>m</em></span> is the
number of neurons in the current layer. We also found that the bias
vector must be of length <span
class="math inline"><em>m</em></span>.</p>
<p>Let’s make functions that automate the entire network creation
process — including initializing weight and bias vectors of the proper
size. Let’s start with a function <code>initialize</code>, that
essentially just creates placeholder <code>Layer</code> structures and
lets the user determine the activation function and size of each
layer:</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">-- Initialize the network. Note that the dimensions of the weight and bias matrices aren&#39;t calculated here yet.</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co">-- This only initializes the layer structures with their intended numbers of neurons and activation functions.</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">init</span><span class="ot"> ::</span> [<span class="dt">Int</span>] <span class="ot">-&gt;</span> [<span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">R</span>] <span class="ot">-&gt;</span> <span class="dt">Network</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">init</span> sizes activations <span class="ot">=</span> <span class="fu">map</span> (\(x, a) <span class="ot">-&gt;</span> <span class="dt">Layer</span>{weights <span class="ot">=</span> (<span class="dv">0</span><span class="op">&gt;&lt;</span><span class="dv">0</span>) [], biases <span class="ot">=</span> (<span class="dv">0</span><span class="op">&gt;&lt;</span><span class="dv">0</span>) [], sz <span class="ot">=</span> x, activation <span class="ot">=</span> a}) <span class="op">$</span> <span class="fu">zip</span> sizes activations</span></code></pre></div>
<p>So we take a list of integers for the layer sizes, and a list of
activation functions of the form <code>R -&gt; R</code> (or <span
class="math inline">ℝ → ℝ</span>). <code>zip</code> converts the two
lists into one list of tuples <code>[(Int, R -&gt; R)]</code>. We map
over that list and create a layer type for each element.</p>
<p>// Let’s make a function that automatically initializes the weights
and biases for a network of any size. Let’s call it
<code>fit</code>.</p>
<p>Now that we have the core structure of the network defined, let’s get
to the cool part — how do we actually get the network to learn?</p>
<h2 id="how-neural-networks-actually-learn">How Neural Networks Actually
Learn</h2>
<p>The information flows forward in a neural network by a process called
<strong>forward propagation</strong>. This is how the neural network
runs the information through to ultimately compute the end result. But
forward propagation isn’t very useful by itself. There must be some
algorithm that keeps updating our parameters <span
class="math inline"><em>w</em>, <em>b</em></span> for every neuron to
get closer to a better fit. That sounds familiar — and indeed it is; we
can use gradient descent, just like we previously have! In particular,
we want to use an application of gradient descent called
<strong>backpropagation</strong>.</p>
<h3 id="forward-propagation">Forward Propagation</h3>
<p>Essentially forward propagation just does the two following steps: 1.
Does computations and calls the activation function on a layer. 2. Take
the output of that layer and treat it as the input to the next layer. 3.
Repeat 1-2 from the first layer all the way to the output layer.</p>
<p>We can view forward propagation as a function composed of all the
layer computation functions. Consider us having three layers <span
class="math inline"><em>l</em><sub>1</sub></span>, <span
class="math inline"><em>l</em><sub>2</sub></span> and <span
class="math inline"><em>l</em><sub>3</sub></span>. Let <span
class="math inline"><em>f</em><sub><em>n</em></sub></span> represent the
entire computation (the linear function + the activation function) for
some layer <span
class="math inline"><em>l</em><sub><em>n</em></sub></span>. We can now
express the entire forward propagation algorithm as: <span
class="math display"><em>f</em><sub>3</sub>(<em>f</em><sub>2</sub>(<em>f</em><sub>1</sub>(<strong>x</strong>)))) = (<em>f</em><sub>3</sub>∘<em>f</em><sub>2</sub>∘<em>f</em><sub>1</sub>)(<strong>x</strong>)</span></p>
<p>That’s all it is. So we just repeat the same process on every layer,
using the output of the previous layer. To make notation consistent,
<span class="math inline"><em>l</em><sub>0</sub></span> is often used to
represent the input vector <span
class="math inline"><strong>x</strong></span> as the <strong>input
layer</strong>.</p>
<p>I find this quite funny — the entire forward propagation function can
be expressed using just two words in Haskell. Literally, just two words.
Let’s take a slightly longer approach first and simplify from there for
optimized understanding.</p>
<p>In Haskell, there is a function <code>foldl</code> that essentially
maps over a list with an accumulator. It’s a bit like JavaScript’s
<code>reduce</code> function. Let’s take a look at the type of
<code>foldl</code>:</p>
<div class="sourceCode" id="cb4"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>ghci<span class="op">&gt;</span> <span class="op">:</span><span class="kw">type</span> <span class="fu">foldl</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">foldl</span><span class="ot"> ::</span> <span class="dt">Foldable</span> t <span class="ot">=&gt;</span> (b <span class="ot">-&gt;</span> a <span class="ot">-&gt;</span> b) <span class="ot">-&gt;</span> b <span class="ot">-&gt;</span> t a <span class="ot">-&gt;</span> b</span></code></pre></div>
<p>The <code>Foldable t =&gt;</code> part means that the generic type
<code>t</code> has to be a part of the <code>Foldable</code> typeclass.
It’s just like Rust’s trait system (except it was established way back
in the 80s). So the arguments are: 1. a function that takes two
arguments of types <code>b</code> and <code>a</code> (in our case,
<code>a</code> and <code>b</code> will be <code>Layer</code> and the
input vector <code>Matrix R</code> respectively) and returns another
<code>b</code> (<code>Layer</code>, that is), 2. a <code>Foldable</code>
(in our case a list) of values of type <code>a</code> (a list of
<code>Layer</code> — the <code>Network</code> type), 3. and the inial
accumulator value (the input vector).</p>
<p>Let’s look into how this compares with JavaScript’s
<code>reduce</code> function. If you’ve seen the code for getting the
sum of an array in JavaScript:</p>
<div class="sourceCode" id="cb5"><pre
class="sourceCode javascript"><code class="sourceCode javascript"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>[<span class="dv">1</span><span class="op">,</span> <span class="dv">2</span><span class="op">,</span> <span class="dv">3</span>]<span class="op">.</span><span class="fu">reduce</span>((a<span class="op">,</span> b) <span class="kw">=&gt;</span> a <span class="op">+</span> b)</span></code></pre></div>
<p>you will understand how our implementation works. The difference is
that <code>foldl</code> is explicitly specified the initial accumulator
value, whereas <code>reduce</code> just implicitly uses the first
element in the array as the initial accumulator.</p>
<p>Here’s how to replicate the array sum in Haskell using
<code>foldl</code>:</p>
<div class="sourceCode" id="cb6"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">foldl</span> (\a b <span class="ot">-&gt;</span> a <span class="op">+</span> b) <span class="dv">0</span> [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>]</span></code></pre></div>
<p>We use <code>0</code> as the initial accumulator value. Otherwise the
code looks very similar.</p>
<p>Well, enough of that. Let’s get into the actual forward propagation
implementation now. Here’s how it looks:</p>
<div class="sourceCode" id="cb7"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ot">forwardProp ::</span> <span class="dt">Matrix</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Matrix</span> <span class="dt">R</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>forwardProp input net <span class="ot">=</span> <span class="fu">foldl</span> (\inp layer <span class="ot">-&gt;</span> activate inp layer) input net</span></code></pre></div>
<p>We take two arguments — the initial input vector and the network. We
fold over a list of <code>Layer</code>s with an accumulator of type
<code>Matrix R</code>, which will be the input to the current layer
we’re folding over. Due to Haskell’s currying (<a href="">read more
here</a>), we can simplify this code a bit:</p>
<div class="sourceCode" id="cb8"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ot">forwardProp ::</span> <span class="dt">Matrix</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Matrix</span> <span class="dt">R</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>forwardProp input net <span class="ot">=</span> <span class="fu">foldl</span> (activate) input net</span></code></pre></div>
<p>Haskell is able to recognize that we will directly be calling another
function with our lambda function’s parameters, in the same order that
in which they’re defined. As a matter of fact, we can take this even
further and utilize this on the actual function declaration:</p>
<div class="sourceCode" id="cb9"><pre
class="sourceCode haskell"><code class="sourceCode haskell"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ot">forwardProp ::</span> <span class="dt">Matrix</span> <span class="dt">R</span> <span class="ot">-&gt;</span> <span class="dt">Network</span> <span class="ot">-&gt;</span> <span class="dt">Matrix</span> <span class="dt">R</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>forwardProp <span class="ot">=</span> <span class="fu">foldl</span> activate</span></code></pre></div>
<p>Think about it. If <code>forwardProp x y = foldl activate x y</code>,
should it not follow that <code>forwardProp = foldl activate</code>?</p>
<p>Anyway, THAT is a concise forward propagation function. Tell me that
is not beautiful. You may see why I find it so fun to write Haskell
code.</p>
</body>
</html>
